{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOG_CACHE = {}\n",
    "def add_to_cache (img, HOG) :\n",
    "    h = hash(np.array(img).tobytes())\n",
    "    HOG_CACHE[h] = HOG\n",
    "\n",
    "def get_from_cache (img) :\n",
    "    key = hash(np.array(img).tobytes())\n",
    "    return HOG_CACHE[key]\n",
    "\n",
    "def in_cache (img) :\n",
    "    key = hash(np.array(img).tobytes())\n",
    "    return key in HOG_CACHE\n",
    "\n",
    "\n",
    "def metric_chi2 (x, y) :\n",
    "    return (np.sum((x - y)**2 / (x + y)) / 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from os import listdir\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import correlate\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "\n",
    "import skimage.io as skio\n",
    "import skimage.util as skutil\n",
    "import skimage.color as skolor\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def read_collection (path, read_gray=False) :\n",
    "    ''' Read a collection of images.\n",
    "    \n",
    "    Arguments:\n",
    "    path (string)         -- the directory path\n",
    "    [read_gray] (boolean) -- argument that gets passed to skimage.io.imread\n",
    "                             that determines if the images are converted\n",
    "                             to grayscale after being read\n",
    "                   \n",
    "    Returns:\n",
    "    ims (ndarray) --  a list where each element is a numpy array \n",
    "                      representing a image\n",
    "    '''\n",
    "    \n",
    "    files = listdir(path)\n",
    "    ims = []\n",
    "    for fname in files :\n",
    "        try :\n",
    "            afname = path + '/' + fname\n",
    "            im = skio.imread(afname, read_gray)\n",
    "            ims.append(im)\n",
    "        except Exception as e :\n",
    "            print('File is not a supported image format')\n",
    "    \n",
    "    return ims\n",
    "\n",
    "def normalize (histograms) :\n",
    "    ''' Normalizes a histogram so that the sum of its elements equals 1.\n",
    "    \n",
    "    Arguments:\n",
    "    histograms (ndarray) -- one or a collection of histograms\n",
    "    \n",
    "    Returns:\n",
    "    nhist (ndarray) -- the normalized histogram(s)\n",
    "    '''\n",
    "    \n",
    "    sums = np.sum(histograms, axis=1)\n",
    "    histograms = np.rollaxis(histograms, 1)\n",
    "    histograms = histograms / sums\n",
    "    histograms = np.rollaxis(histograms, 1)\n",
    "    \n",
    "    return histograms\n",
    "\n",
    "def gradient (img) :\n",
    "    ''' Computes the gradient of an image.\n",
    "    \n",
    "    The gradient is a 3D matrix where the first layer stores the \n",
    "    magnitude, while the second layer stores its orientation (in degrees).\n",
    "    \n",
    "    Arguments:\n",
    "    img (ndarray) -- the matrix for which the gradient is computed\n",
    "    \n",
    "    Returns (NxMx2 ndarray) -- the gradient where N and M are the height and width\n",
    "                               of img\n",
    "    '''\n",
    "    img = skolor.rgb2gray(img)\n",
    "    grad = np.zeros(img.shape+(2,))\n",
    "    \n",
    "    f = np.array([[0, 0, 0], [-1, 0, 1], [0, 0, 0]])\n",
    "    gradx = correlate(img, f, origin=1)\n",
    "    grady = correlate(img, f.transpose(), origin=1)\n",
    "    \n",
    "    orient = np.degrees(np.arctan2(grady, gradx))+180\n",
    "    mag = np.sqrt(gradx**2 + grady**2)\n",
    "    \n",
    "    grad[:,:,0] = mag\n",
    "    grad[:,:,1] = orient\n",
    "    \n",
    "    return grad\n",
    "    \n",
    "def impatches (img, grid_shape, patch_shape=(16, 16)) :\n",
    "    ''' Divides the source image into equale spaced windows.\n",
    "    \n",
    "    The division is made using skimage.util.view_as_windows\n",
    "    that creates a sliding window over the specified image.\n",
    "    \n",
    "    The windows may overlap if the grid_shape is big enough.\n",
    "    \n",
    "    Arguments:\n",
    "    img (ndarray)         -- the matrix which will get sliced\n",
    "    grid_shape (tuple)    -- the number of windows along each \n",
    "                             dimension\n",
    "    [patch_shape] (tuple) -- the size of each window\n",
    "    \n",
    "    Returns:\n",
    "    patches (axbxNxM[xC] ndarray) -- the window grid where a and b\n",
    "                                     are sizes of grid_shape and \n",
    "                                     N, M and C are the height, width\n",
    "                                     and number of channels of the image\n",
    "                                     (if applicable)\n",
    "    '''\n",
    "\n",
    "    n_vpatch = grid_shape[0]\n",
    "    n_hpatch = grid_shape[1]\n",
    "    imh = img.shape[0]\n",
    "    imw = img.shape[1]\n",
    "    \n",
    "    if img.ndim == 3 :\n",
    "        patch_shape = patch_shape + (3,)\n",
    "    \n",
    "    patchw = patch_shape[0]\n",
    "    patchh = patch_shape[1]\n",
    "    interval = (floor((imh-patchh)/(n_hpatch-1)),\n",
    "                floor((imw-patchw)/(n_vpatch-1)))\n",
    "    if img.ndim == 3 :\n",
    "        interval = interval + (3,)\n",
    "    \n",
    "    patches = skutil.view_as_windows(np.ascontiguousarray(img), \n",
    "                                     patch_shape, interval)\n",
    "    \n",
    "    if patches.shape[2] == 1 :\n",
    "        patches = patches.squeeze(2)\n",
    "    return patches\n",
    "\n",
    "def describe_HOG (go_pfield) :\n",
    "    ''' Given an image, it generates its HOG feature vector.\n",
    "    \n",
    "    The descriptor returned is a 128-dimensional vector, which\n",
    "    has been obtained by concatenating 16 histograms of 8 bins each.\n",
    "    \n",
    "    The histograms are obtained by dividing the field into an evenly\n",
    "    spaced out grid, and for each window we obtain 16 histograms by\n",
    "    further dividing it into a 4x4 grid.\n",
    "    \n",
    "    The histograms count the orientations of the gradients.\n",
    "    \n",
    "    Arguments:\n",
    "    go_pfield (ndarray) -- the gradient orientation matrix (this\n",
    "                           has been obtained with the gradient function\n",
    "                           and is the second layer of the returned array)\n",
    "                           \n",
    "    Returns:\n",
    "    hog_descriptor (1x128 ndarray) -- a 128-dimensional vector\n",
    "    '''\n",
    "    pshape = go_pfield[0,0].shape\n",
    "    gshape = (4,4)\n",
    "    cshape = (round(pshape[0]/gshape[0]),\n",
    "              round(pshape[1]/gshape[1]))\n",
    "    \n",
    "    \n",
    "    hog_vector = []\n",
    "    hog_descriptor = []\n",
    "    for row in go_pfield :\n",
    "        for patch in row : \n",
    "            cell_grid = impatches(patch, gshape, cshape)\n",
    "            for crow in cell_grid :\n",
    "                for cell in crow :\n",
    "                    hist = np.histogram(cell, bins=8, range=(0, 360))[0]\n",
    "                    hog_vector.append(hist)\n",
    "            hog_descriptor.append(np.array(hog_vector).flatten())\n",
    "            hog_vector = []\n",
    "    \n",
    "    hog_descriptor = np.array(hog_descriptor)\n",
    "    \n",
    "    return hog_descriptor\n",
    "\n",
    "def generate_vocabulary (collection, k, iter_max=100, grid_shape=(10, 10), patch_shape=16) :\n",
    "    ''' Generate a visual words vocabulary.\n",
    "    \n",
    "    Because describing an image takes time and vocabulary genertion\n",
    "    might be necesarry more than once, we store the HOG descriptor\n",
    "    of each image into a cache from which we later retrieve the\n",
    "    descriptors if they've been computed before.\n",
    "    \n",
    "    Arguments:\n",
    "    collection (ndarray)   -- collection of images from which a\n",
    "                              vocabulary will be generated\n",
    "    k (scalar)             -- the number of words the vocabulary\n",
    "                              will contain\n",
    "    [iter_max] (scalar)    -- the number of iterations at which the\n",
    "                              k-means algorithm should stop\n",
    "    [grid_shape] (tuple)   -- the shape of the grid into which\n",
    "                              each image will be divided when describing it\n",
    "    [patch_shape] (scalar) -- the size of the windows of the grid (squares)\n",
    "    \n",
    "    Returns:\n",
    "    kmeans (KMeans) -- KMeans object which can be used to obtain the words\n",
    "                       and their labels.\n",
    "                       words: KMeans.cluster_centers_\n",
    "                       labels: KMeans.labels_\n",
    "                       The labels are relevant to the collection, each\n",
    "                       label corresponding to one image.\n",
    "    '''\n",
    "    stime = time.clock()\n",
    "    descriptors = []\n",
    "    for im in collection :\n",
    "        if not in_cache(im) :\n",
    "            gof = gradient(im)[:,:,1]\n",
    "            gopf = impatches(gof, grid_shape)\n",
    "            desc = describe_HOG(gopf)\n",
    "            \n",
    "            add_to_cache(im, desc)\n",
    "        else :\n",
    "            desc = get_from_cache(im)\n",
    "        \n",
    "        descriptors.append(desc)\n",
    "    descriptors = np.array(descriptors)\n",
    "    \n",
    "    kmeans = KMeans(k, max_iter=iter_max)\n",
    "    descriptors = descriptors.reshape(-1, 128)\n",
    "    kmeans.fit(descriptors)\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "\n",
    "def histogram_BOVW (imgs, vwdict, grid_shape=(10, 10), normalized=False) :\n",
    "    ''' Generates the Bag of Visual Words histogram that describes an image.\n",
    "    \n",
    "    It does that by assigning each image's HOG descriptors to a \n",
    "    cluster in the dictionarie's \"word space\". Afterwards, it counts\n",
    "    how many times each label appears for an image.\n",
    "    \n",
    "    Arguments:\n",
    "    imgs (ndarray)       -- a collection of images\n",
    "    vwdict (KMeans)      -- dictionary generated using generate_vocabulary\n",
    "    [grid_shape] (tuple) -- the size of the grid into which the images\n",
    "                            will be split when describing them using HOG\n",
    "                            \n",
    "    Returns:\n",
    "    hists (NxK ndarray) -- a collection of histograms that describe\n",
    "                           each image in the provided collection.\n",
    "                           N is the number of images and K is the number\n",
    "                           of words.\n",
    "    '''\n",
    "    desc = []\n",
    "    for img in imgs : \n",
    "        if not in_cache(img) :\n",
    "            gof = gradient(img)[:,:,1]\n",
    "            gopf = impatches(gof, grid_shape)\n",
    "            dc = describe_HOG(gopf)\n",
    "            add_to_cache(img, dc)\n",
    "        else : \n",
    "            dc = get_from_cache(img)\n",
    "        desc.append(dc)\n",
    "    # we reshape it so that each line contains a descriptor\n",
    "    # instead of grid_shape width * grid_shape height\n",
    "    desc = np.array(desc).reshape(-1, 128)\n",
    "        \n",
    "    \n",
    "    labels = vwdict.predict(desc)\n",
    "    nwords = len(vwdict.cluster_centers_)\n",
    "    # we reshape the labels so that each line contains\n",
    "    # the labels for one image\n",
    "    labels = labels.reshape(-1, grid_shape[0]*grid_shape[1])\n",
    "    hists = [] \n",
    "    for l in labels : \n",
    "        hist = np.histogram(l, nwords, range=(0, nwords), density=normalized)[0]\n",
    "        hists.append(hist)\n",
    "    hists = np.array(hists)\n",
    "    \n",
    "    return hists\n",
    "\n",
    "\n",
    "def classify_CN (collection, phcoll_BOVW, nhcoll_BOVW, metric='euclidean') :\n",
    "    ''' Classifies the collection using a nearest neighbour classifier.\n",
    "    \n",
    "    Arguments:\n",
    "    collection (ndarray)  -- collection of BOVW histograms\n",
    "    phcoll_BOVW (ndarray) -- training collection that represent \n",
    "                             positive samples of the data\n",
    "    nhcoll_BOVW (ndarray) -- training collection that represent\n",
    "                             negative samples of the data\n",
    "    \n",
    "    Returns:\n",
    "    labels (1xN ndarray) -- labels for each element in the collection.\n",
    "                            A True label classifies the element as \n",
    "                            being similar to the ones in the positive\n",
    "                            training set, while a False label \n",
    "                            classifies it as being part of the negative\n",
    "                            set.\n",
    "    '''\n",
    "    collection = normalize(collection)\n",
    "    phcoll_BOVW = normalize(phcoll_BOVW)\n",
    "    nhcoll_BOVW = normalize(nhcoll_BOVW)\n",
    "    \n",
    "#     pdistmat = cdist(collection, phcoll_BOVW, metric)\n",
    "#     ndistmat = cdist(collection, nhcoll_BOVW, metric)\n",
    "    \n",
    "#     pmindist_mat = np.min(pdistmat, axis=1)\n",
    "#     nmindist_mat = np.min(ndistmat, axis=1)\n",
    "    \n",
    "#     labels = np.less(pmindist_mat, nmindist_mat)\n",
    "\n",
    "    pncollection = np.vstack((phcoll_BOVW, nhcoll_BOVW))\n",
    "    distmat = cdist(collection, pncollection, metric)\n",
    "    \n",
    "    distmat = np.argsort(distmat, axis=1)[:,:5]\n",
    "    labels = np.sum(distmat < phcoll_BOVW.shape[0], axis=1) >= 3\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def natdist_parameters (collection) :\n",
    "    ''' Returns the infered parameters of a natural distribution\n",
    "    \n",
    "    If the supplied data is a matrix, the means and deviations\n",
    "    will be computed along each column.\n",
    "    \n",
    "    Arguments:\n",
    "    collection (ndarray) -- array (or matrix) that contain\n",
    "                            the data points.\n",
    "    \n",
    "    Returns:\n",
    "    means      (scalar, ndarray) -- the mean of the data set\n",
    "    deviations (scalar, ndarray) -- the deviations of the data set.\n",
    "                                    If the value was a 0, it was replaced\n",
    "                                    with a 1.\n",
    "    '''\n",
    "    means = np.mean(collection, axis=0)\n",
    "    \n",
    "    deviations = np.std(collection, axis=0)\n",
    "    deviations[deviations==0] = 1\n",
    "    \n",
    "    return means,deviations\n",
    "\n",
    "def classify_bayes (collection, phists, nhists) :\n",
    "    ''' Classifies the collection using a Naive Bayes Classifier\n",
    "    \n",
    "    The supplied histograms shouldn't be normalized.\n",
    "    \n",
    "    Arguments:\n",
    "    collection (ndarray)  -- collection of BOVW histograms\n",
    "    phists (ndarray)      -- training collection that represent \n",
    "                             positive samples of the data\n",
    "    nhists (ndarray)      -- training collection that represent\n",
    "                             negative samples of the data\n",
    "                             \n",
    "                             \n",
    "    Returns:\n",
    "    labels (1xN ndarray) -- labels for each element in the collection.\n",
    "                            A True label classifies the element as \n",
    "                            being similar to the ones in the positive\n",
    "                            training set, while a False label \n",
    "                            classifies it as being part of the negative\n",
    "                            set.\n",
    "    '''\n",
    "    pmu, psigma = natdist_parameters(phists)\n",
    "    nmu, nsigma = natdist_parameters(nhists)\n",
    "    \n",
    "    p_hist_masina = np.prod(norm.pdf(collection, pmu, psigma), axis=1)\n",
    "    p_hist_nonmasina = np.prod(norm.pdf(collection, nmu, nsigma), axis=1)\n",
    "\n",
    "    p_masina_hist = p_hist_masina / (p_hist_masina + p_hist_nonmasina)\n",
    "    \n",
    "    return p_masina_hist > 0.5\n",
    "\n",
    "def classify_svm (collection, phists, nhists) :\n",
    "    collection = normalize(collection)\n",
    "    phists = normalize(phists)\n",
    "    nhists = normalize(nhists)    \n",
    "    cl = SVC()\n",
    "    \n",
    "    tdata = np.concatenate((phists, nhists), axis=0)\n",
    "    labels = np.concatenate((np.ones(phists.shape[0]), np.zeros(nhists.shape[0])))\n",
    "    cl.fit(tdata, labels)\n",
    "    \n",
    "    return cl.predict(collection)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcollection = read_collection('data/masini-exempleAntrenare-pozitive/')\n",
    "ncollection = read_collection('data/masini-exempleAntrenare-negative/')\n",
    "pncollection = pcollection + ncollection\n",
    "testpcollection = read_collection('data/masini-exempleTestare-pozitive/')\n",
    "testncollection = read_collection('data/masini-exempleTestare-negative/')\n",
    "\n",
    "print('Starting generation of dictionary')\n",
    "stime = time.clock()\n",
    "\n",
    "dictsizes = [5, 10, 25, 50, 100, 200, 500, 1000]\n",
    "dictionary = []\n",
    "for k in dictsizes :\n",
    "    print('Generating vocabulary (k={})...'.format(k))\n",
    "    v = generate_vocabulary(pncollection, k)\n",
    "    dictionary.append(v)\n",
    "    print('Done')\n",
    "print('Done generating the dictionary: {}ms'.format(time.clock() - stime))\n",
    "print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def efficacy (labels, target) :\n",
    "    return np.sum(labels == target) / labels.size\n",
    "\n",
    "def test_method (method, name) :\n",
    "    print(name + '...')\n",
    "    stime = time.clock()\n",
    "    plabels = method(tphists, phists, nhists)\n",
    "    t = time.clock() - stime\n",
    "    nlabels = method(tnhists, phists, nhists)\n",
    "    e = efficacy(plabels, 1)/2 + efficacy(nlabels, 0)/2\n",
    "    print('Done: {}ms (50 data points), {} efficacy'.format(t, e))\n",
    "    \n",
    "    return t, e\n",
    "\n",
    "stats = {\n",
    "    'nns' : {\n",
    "        'times': [],\n",
    "        'efficacies': []\n",
    "    },\n",
    "    'bayes' : {\n",
    "        'times': [],\n",
    "        'efficacies': []\n",
    "    },\n",
    "    'svm' : {\n",
    "        'times': [],\n",
    "        'efficacies': []\n",
    "    }\n",
    "}\n",
    "print('Starting profiling')\n",
    "for vocab in dictionary : \n",
    "    print('='*20,'='*20, sep='\\n')\n",
    "    print('Vocabulary size: {}'.format(len(vocab.cluster_centers_)))\n",
    "    print('='*20,'='*20, sep='\\n')\n",
    "    print('Starting description of sample data...')\n",
    "    stime = time.clock()\n",
    "    phists = histogram_BOVW(pcollection, vocab)\n",
    "    nhists = histogram_BOVW(ncollection, vocab)\n",
    "    pnhists = histogram_BOVW(pncollection, vocab)\n",
    "    tphists = histogram_BOVW(testpcollection, vocab)\n",
    "    tnhists = histogram_BOVW(testncollection, vocab)\n",
    "    print('Done: {}ms'.format(time.clock() - stime))\n",
    "    print('='*20)\n",
    "    \n",
    "    t, e = test_method(classify_CN, 'Nearest Neighbour Classifier')\n",
    "    stats['nns']['times'].append(t)\n",
    "    stats['nns']['efficacies'].append(e)\n",
    "    \n",
    "    t, e = test_method(classify_bayes, 'Naive Bayes Classifier')\n",
    "    stats['bayes']['times'].append(t)\n",
    "    stats['bayes']['efficacies'].append(e)\n",
    "    \n",
    "    t, e = test_method(classify_svm, 'Support Vector Machine')\n",
    "    stats['svm']['times'].append(t)\n",
    "    stats['svm']['efficacies'].append(e)\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(stats['nns']['times']))\n",
    "print(np.array(stats['nns']['efficacies']), end='\\n\\n\\n')\n",
    "\n",
    "print(np.array(stats['bayes']['times']))\n",
    "print(np.array(stats['bayes']['efficacies']), end='\\n\\n\\n')\n",
    "\n",
    "print(np.array(stats['svm']['times']))\n",
    "print(np.array(stats['svm']['efficacies']), end='\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
